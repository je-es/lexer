{"version":3,"sources":["../lib/lexer.ts"],"sourcesContent":["// lexer.ts — Fundamental lexical analyzer that transforms\n//            source text into structured tokens with type and position information.\n//\n// repo   : https://github.com/je-es/lexer\n// author : https://github.com/maysara-elshewehy\n//\n// Developed with ❤️ by Maysara.\n\n\n\n// ╔════════════════════════════════════════ TYPE ════════════════════════════════════════╗\n\n    /** Represents a token with type, value and position information */\n    export interface Token {\n        type        : string;\n        value       : string | null;\n        range       : Range;\n    }\n\n    /** Represents a position in the source text */\n    export interface Position {\n        line        : number;\n        col         : number;\n        offset      : number;\n    }\n\n    /** Represents a range in the source text */\n    export interface Range {\n        start       : Position;\n        end         : Position;\n    }\n\n    /** Configuration for a lexer rule defining how to match and process tokens */\n    export interface RuleConfig {\n        match       : RegExp;\n        value      ?: (text: string) => string;\n        lineBreaks ?: boolean;\n    }\n\n    /** Defines a rule that can be a string, RegExp, string array, or RuleConfig */\n    export type Rule = string | RegExp | string[] | RuleConfig;\n\n    /** Collection of named rules for tokenization */\n    export interface Rules {\n        [key: string]: Rule;\n    }\n\n    export const error = Symbol('error');\n\n// ╚══════════════════════════════════════════════════════════════════════════════════════╝\n\n\n\n// ╔════════════════════════════════════════ CORE ════════════════════════════════════════╗\n\n    /**\n     * Lexical analyzer that converts source text into tokens\n     *\n     * The lexer processes input text according to defined rules and produces\n     * a stream of tokens with type and position information.\n    */\n    export class Lexer {\n        private fastRegex       : RegExp | null                         = null;\n        private ruleTypes       : string[]                              = [];\n        private ruleTransforms  : (((text: string) => string) | null)[] = [];\n        private ruleLineBreaks  : boolean[]                             = [];\n        private buffer          : string                                = '';\n        private position        : number                                = 0;\n        private line            : number                                = 1;\n        private col             : number                                = 1;\n        private length          : number                                = 0;\n\n        constructor(rules: Rules) {\n            this.compileRules(rules);\n        }\n\n        private compileRules(rules: Rules): void {\n            const patterns      : string[]                              = [];\n            const types         : string[]                              = [];\n            const transforms    : (((text: string) => string) | null)[] = [];\n            const lineBreaks    : boolean[]                             = [];\n\n            for (const [name, rule] of Object.entries(rules)) {\n                let pattern         : string;\n                let transform       : ((text: string) => string) | null = null;\n                let hasLineBreaks   : boolean                           = false;\n\n                if (typeof rule === 'symbol' && rule === error) {\n                    // Skip error rule for now - handle in fallback\n                    continue;\n                } else if (typeof rule === 'string') {\n                    pattern = this.escapeRegex(rule);\n                } else if (rule instanceof RegExp) {\n                    pattern = rule.source;\n                } else if (Array.isArray(rule)) {\n                    // Keywords with word boundaries - optimize common case\n                    if (rule.length === 1) {\n                        pattern = `${this.escapeRegex(rule[0])}(?![a-zA-Z0-9_])`;\n                    } else {\n                        pattern = `(?:${rule.map(k => this.escapeRegex(k)).join('|')})(?![a-zA-Z0-9_])`;\n                    }\n                } else {\n                    // RuleConfig\n                    const config = rule as RuleConfig;\n                    pattern = config.match.source;\n                    transform = config.value || null;\n                    hasLineBreaks = config.lineBreaks || false;\n                }\n\n                patterns.push(`(${pattern})`);\n                types.push(name);\n                transforms.push(transform);\n                lineBreaks.push(hasLineBreaks);\n            }\n\n            // Compile single mega-regex for maximum performance\n            if (patterns.length > 0) {\n                this.fastRegex = new RegExp(patterns.join('|'), 'gy');\n            }\n\n            // Store as flat arrays for better cache performance\n            this.ruleTypes = types;\n            this.ruleTransforms = transforms;\n            this.ruleLineBreaks = lineBreaks;\n        }\n\n        private escapeRegex(str: string): string {\n            return str.replace(/[.*+?^${}()|[\\]\\\\]/g, '\\\\$&');\n        }\n\n        setup(input: string): void {\n            this.buffer = input;\n            this.length = input.length;\n            this.position = 0;\n            this.line = 1;\n            this.col = 1;\n\n            if (this.fastRegex) {\n                this.fastRegex.lastIndex = 0;\n            }\n        }\n\n        next(): Token | undefined {\n            // Early exit for end of input\n            if (this.position >= this.length) {\n                return undefined;\n            }\n\n            if (this.fastRegex) {\n                this.fastRegex.lastIndex = this.position;\n                const match = this.fastRegex.exec(this.buffer);\n\n                if (match && match.index === this.position) {\n                    // Find which group matched - optimized loop\n                    let ruleIndex = 0;\n                    let text : string | null = '';\n\n                    // Start from index 1 (skip full match)\n                    for (let i = 1; i < match.length; i++) {\n                        if (match[i] !== undefined) {\n                            ruleIndex = i - 1;\n                            text = match[i];\n                            break;\n                        }\n                    }\n\n                    const startLine = this.line;\n                    const startCol = this.col;\n                    const startPos = this.position;\n                    const textLength = text.length;\n\n                    // Update position\n                    this.position += textLength;\n\n                    // Optimized line/col tracking\n                    if (this.ruleLineBreaks[ruleIndex]) {\n                        // Fast newline counting\n                        for (let i = 0; i < textLength; i++) {\n                            if (text.charCodeAt(i) === 10) { // '\\n'\n                                this.line++;\n                                this.col = 1;\n                            } else {\n                                this.col++;\n                            }\n                        }\n                    } else {\n                        this.col += textLength;\n                    }\n\n                    // Apply transformation if exists\n                    const transform = this.ruleTransforms[ruleIndex];\n                    const value : string | null = transform ? transform(text) : text;\n\n                    return {\n                        type        : this.ruleTypes[ruleIndex],\n                        value,\n                        range       : {\n                            start: { line: startLine, col: startCol, offset: startPos },\n                            end  : { line: this.line, col: this.col, offset: this.position }\n                        }\n                    };\n                }\n            }\n\n            // Fallback: error token\n            const char = this.buffer[this.position];\n            const token = {\n                type        : 'error',\n                value       : char,\n                range       : {\n                    start: { line: this.line, col: this.col, offset: this.position },\n                    end  : { line: this.line, col: this.col, offset: this.position }\n                }\n            };\n\n            this.position++;\n            this.col++;\n\n            return token;\n        }\n\n        *[Symbol.iterator](): Iterator<Token> {\n            let token: Token | undefined;\n            while ((token = this.next()) !== undefined) {\n                yield token;\n            }\n        }\n    }\n\n// ╚══════════════════════════════════════════════════════════════════════════════════════╝\n\n\n\n// ╔════════════════════════════════════════ MAIN ════════════════════════════════════════╗\n\n    /**\n     * Tokenizes source code using the provided rules\n     *\n     * @param source    - The source text to tokenize\n     * @param rules     - Rules defining how to break the source into tokens\n     *\n     * @returns Array of tokens extracted from the source\n    */\n    export function tokenize(source: string, rules: Rules): Token[] {\n        const sourceLength = source.length;\n\n        // Early return for empty input\n        if (sourceLength === 0) { return []; }\n\n        // Use regular array and let JavaScript handle resizing\n        const tokens: Token[] = [];\n\n        // Initialize lexer with compiled rules\n        const lexer = new Lexer(rules);\n\n        // Setup lexer with input\n        lexer.setup(source);\n\n        // Optimized token iteration - avoid iterator overhead\n        let token = lexer.next();\n        while (token !== undefined) {\n            tokens.push({\n                type    : token.type,\n                value   : token.value!.length ? token.value : null,\n                range   : token.range\n            });\n\n            // Stop on error to match original behavior\n            if (token.type === \"error\") break;\n\n            token = lexer.next();\n        }\n\n        return tokens;\n    }\n\n// ╚══════════════════════════════════════════════════════════════════════════════════════╝"],"mappings":";AA+CW,IAAM,QAAQ,OAAO,OAAO;AAc5B,IAAM,QAAN,MAAY;AAAA,EAWf,YAAY,OAAc;AAV1B,SAAQ,YAA0D;AAClE,SAAQ,YAA0D,CAAC;AACnE,SAAQ,iBAA0D,CAAC;AACnE,SAAQ,iBAA0D,CAAC;AACnE,SAAQ,SAA0D;AAClE,SAAQ,WAA0D;AAClE,SAAQ,OAA0D;AAClE,SAAQ,MAA0D;AAClE,SAAQ,SAA0D;AAG9D,SAAK,aAAa,KAAK;AAAA,EAC3B;AAAA,EAEQ,aAAa,OAAoB;AACrC,UAAM,WAAwD,CAAC;AAC/D,UAAM,QAAwD,CAAC;AAC/D,UAAM,aAAwD,CAAC;AAC/D,UAAM,aAAwD,CAAC;AAE/D,eAAW,CAAC,MAAM,IAAI,KAAK,OAAO,QAAQ,KAAK,GAAG;AAC9C,UAAI;AACJ,UAAI,YAAsD;AAC1D,UAAI,gBAAsD;AAE1D,UAAI,OAAO,SAAS,YAAY,SAAS,OAAO;AAE5C;AAAA,MACJ,WAAW,OAAO,SAAS,UAAU;AACjC,kBAAU,KAAK,YAAY,IAAI;AAAA,MACnC,WAAW,gBAAgB,QAAQ;AAC/B,kBAAU,KAAK;AAAA,MACnB,WAAW,MAAM,QAAQ,IAAI,GAAG;AAE5B,YAAI,KAAK,WAAW,GAAG;AACnB,oBAAU,GAAG,KAAK,YAAY,KAAK,CAAC,CAAC,CAAC;AAAA,QAC1C,OAAO;AACH,oBAAU,MAAM,KAAK,IAAI,OAAK,KAAK,YAAY,CAAC,CAAC,EAAE,KAAK,GAAG,CAAC;AAAA,QAChE;AAAA,MACJ,OAAO;AAEH,cAAM,SAAS;AACf,kBAAU,OAAO,MAAM;AACvB,oBAAY,OAAO,SAAS;AAC5B,wBAAgB,OAAO,cAAc;AAAA,MACzC;AAEA,eAAS,KAAK,IAAI,OAAO,GAAG;AAC5B,YAAM,KAAK,IAAI;AACf,iBAAW,KAAK,SAAS;AACzB,iBAAW,KAAK,aAAa;AAAA,IACjC;AAGA,QAAI,SAAS,SAAS,GAAG;AACrB,WAAK,YAAY,IAAI,OAAO,SAAS,KAAK,GAAG,GAAG,IAAI;AAAA,IACxD;AAGA,SAAK,YAAY;AACjB,SAAK,iBAAiB;AACtB,SAAK,iBAAiB;AAAA,EAC1B;AAAA,EAEQ,YAAY,KAAqB;AACrC,WAAO,IAAI,QAAQ,uBAAuB,MAAM;AAAA,EACpD;AAAA,EAEA,MAAM,OAAqB;AACvB,SAAK,SAAS;AACd,SAAK,SAAS,MAAM;AACpB,SAAK,WAAW;AAChB,SAAK,OAAO;AACZ,SAAK,MAAM;AAEX,QAAI,KAAK,WAAW;AAChB,WAAK,UAAU,YAAY;AAAA,IAC/B;AAAA,EACJ;AAAA,EAEA,OAA0B;AAEtB,QAAI,KAAK,YAAY,KAAK,QAAQ;AAC9B,aAAO;AAAA,IACX;AAEA,QAAI,KAAK,WAAW;AAChB,WAAK,UAAU,YAAY,KAAK;AAChC,YAAM,QAAQ,KAAK,UAAU,KAAK,KAAK,MAAM;AAE7C,UAAI,SAAS,MAAM,UAAU,KAAK,UAAU;AAExC,YAAI,YAAY;AAChB,YAAI,OAAuB;AAG3B,iBAAS,IAAI,GAAG,IAAI,MAAM,QAAQ,KAAK;AACnC,cAAI,MAAM,CAAC,MAAM,QAAW;AACxB,wBAAY,IAAI;AAChB,mBAAO,MAAM,CAAC;AACd;AAAA,UACJ;AAAA,QACJ;AAEA,cAAM,YAAY,KAAK;AACvB,cAAM,WAAW,KAAK;AACtB,cAAM,WAAW,KAAK;AACtB,cAAM,aAAa,KAAK;AAGxB,aAAK,YAAY;AAGjB,YAAI,KAAK,eAAe,SAAS,GAAG;AAEhC,mBAAS,IAAI,GAAG,IAAI,YAAY,KAAK;AACjC,gBAAI,KAAK,WAAW,CAAC,MAAM,IAAI;AAC3B,mBAAK;AACL,mBAAK,MAAM;AAAA,YACf,OAAO;AACH,mBAAK;AAAA,YACT;AAAA,UACJ;AAAA,QACJ,OAAO;AACH,eAAK,OAAO;AAAA,QAChB;AAGA,cAAM,YAAY,KAAK,eAAe,SAAS;AAC/C,cAAM,QAAwB,YAAY,UAAU,IAAI,IAAI;AAE5D,eAAO;AAAA,UACH,MAAc,KAAK,UAAU,SAAS;AAAA,UACtC;AAAA,UACA,OAAc;AAAA,YACV,OAAO,EAAE,MAAM,WAAW,KAAK,UAAU,QAAQ,SAAS;AAAA,YAC1D,KAAO,EAAE,MAAM,KAAK,MAAM,KAAK,KAAK,KAAK,QAAQ,KAAK,SAAS;AAAA,UACnE;AAAA,QACJ;AAAA,MACJ;AAAA,IACJ;AAGA,UAAM,OAAO,KAAK,OAAO,KAAK,QAAQ;AACtC,UAAM,QAAQ;AAAA,MACV,MAAc;AAAA,MACd,OAAc;AAAA,MACd,OAAc;AAAA,QACV,OAAO,EAAE,MAAM,KAAK,MAAM,KAAK,KAAK,KAAK,QAAQ,KAAK,SAAS;AAAA,QAC/D,KAAO,EAAE,MAAM,KAAK,MAAM,KAAK,KAAK,KAAK,QAAQ,KAAK,SAAS;AAAA,MACnE;AAAA,IACJ;AAEA,SAAK;AACL,SAAK;AAEL,WAAO;AAAA,EACX;AAAA,EAEA,EAAE,OAAO,QAAQ,IAAqB;AAClC,QAAI;AACJ,YAAQ,QAAQ,KAAK,KAAK,OAAO,QAAW;AACxC,YAAM;AAAA,IACV;AAAA,EACJ;AACJ;AAgBO,SAAS,SAAS,QAAgB,OAAuB;AAC5D,QAAM,eAAe,OAAO;AAG5B,MAAI,iBAAiB,GAAG;AAAE,WAAO,CAAC;AAAA,EAAG;AAGrC,QAAM,SAAkB,CAAC;AAGzB,QAAM,QAAQ,IAAI,MAAM,KAAK;AAG7B,QAAM,MAAM,MAAM;AAGlB,MAAI,QAAQ,MAAM,KAAK;AACvB,SAAO,UAAU,QAAW;AACxB,WAAO,KAAK;AAAA,MACR,MAAU,MAAM;AAAA,MAChB,OAAU,MAAM,MAAO,SAAS,MAAM,QAAQ;AAAA,MAC9C,OAAU,MAAM;AAAA,IACpB,CAAC;AAGD,QAAI,MAAM,SAAS,QAAS;AAE5B,YAAQ,MAAM,KAAK;AAAA,EACvB;AAEA,SAAO;AACX;","names":[]}